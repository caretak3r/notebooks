{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from os import listdir\n",
    "from os import path\n",
    "from os import mkdir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"vix volatility\"\n",
    "url = f\"https://scholar.google.com/scholar?hl=en&as_sdt=0%2C31&q={query}&btnG=\"\n",
    "# create a dir based on the query\n",
    "directory = path.join(\"pdfs\", query.replace(\" \", \"_\"))\n",
    "print(directory)\n",
    "try:\n",
    "    mkdir(directory)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "search_results = soup.find_all(\"div\", attrs={\"class\": \"gs_r gs_or gs_scl\"})\n",
    "print(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs = []\n",
    "for r in search_results:\n",
    "    link = r.find(\"a\")[\"href\"]\n",
    "    if link.endswith(\".pdf\") or link.endswith(\"pdf\"):\n",
    "        print(link)\n",
    "        pdfs.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "\n",
    "def add_eof_marker_to_documents(directory):\n",
    "    EOF_MARKER = b\"%%EOF\"\n",
    "    for files in listdir(directory):\n",
    "        if files.endswith(\".pdf\"):\n",
    "            with open(f\"{directory}/{files}\", \"rb\") as f:\n",
    "                contents = f.read()\n",
    "                if EOF_MARKER in contents:\n",
    "                    print(f\"found EOF_MARKER in {directory}/{files}\")\n",
    "                    contents = contents.replace(EOF_MARKER, b\"\")\n",
    "                    contents = contents + EOF_MARKER\n",
    "                else:\n",
    "                    print(f\"did not find EOF_MARKER in {directory}/{files}\")\n",
    "                    print(contents[-8:])  # see last chars\n",
    "                    contents = contents[:-6] + EOF_MARKER\n",
    "            with open(f\"{directory}/{files}\", \"wb\") as f:\n",
    "                f.write(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in pdfs:\n",
    "    response = requests.get(url)\n",
    "    file_name = url.split(\"/\")[-1]  # get file name\n",
    "\n",
    "    with open(f\"{directory}/{file_name}\", \"wb\") as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "url = \"https://finmasters.com/hedge-fund-letters-to-investors/\"\n",
    "retry_delay = 5  # Time to wait before retrying (in seconds)\n",
    "max_retries = 1  # Maximum number of retries\n",
    "chrome_driver_path = \"/chrome/chromedriver.exe\"  # Update this to the path where you placed the Chrome WebDriver\n",
    "\n",
    "\n",
    "def make_request(url, retries=max_retries):\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            return response\n",
    "        except (requests.exceptions.RequestException, ConnectionError) as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            retries -= 1\n",
    "            if retries > 0:\n",
    "                print(f\"Retrying... ({max_retries - retries + 1} of {max_retries})\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(\"Max retries reached. Skipping this URL.\")\n",
    "                return None\n",
    "\n",
    "\n",
    "response = make_request(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response is not None and response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find all the links to PDFs\n",
    "    pdf_links = soup.find_all(\"a\", href=lambda href: href and href.endswith(\".pdf\"))\n",
    "\n",
    "    # Create a directory to save the PDFs\n",
    "    output_dir = \"pdfs/hedge_fund_letters\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Set up the Selenium WebDriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_experimental_option(\n",
    "        \"prefs\",\n",
    "        {\n",
    "            \"download.default_directory\": os.path.abspath(output_dir),\n",
    "            \"download.prompt_for_download\": False,\n",
    "            \"download.directory_upgrade\": True,\n",
    "            \"plugins.always_open_pdf_externally\": True,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    driver = webdriver.Chrome(executable_path=chrome_driver_path, options=options)\n",
    "\n",
    "    # Download and save each PDF\n",
    "    # TODO: if pdf exists skip\n",
    "    for link in pdf_links:\n",
    "        pdf_url = link[\"href\"]\n",
    "        pdf_name = os.path.join(output_dir, os.path.basename(pdf_url))\n",
    "\n",
    "        try:\n",
    "            driver.get(pdf_url)\n",
    "            time.sleep(5)  # Wait for the download to start\n",
    "        except NoSuchElementException:\n",
    "            print(f\"Failed to download: {pdf_url}\")\n",
    "            print(\"Opening the URL in the default web browser...\")\n",
    "            webbrowser.open(pdf_url)\n",
    "\n",
    "    driver.quit()\n",
    "else:\n",
    "    print(\"Failed to fetch the webpage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paperqa import Docs\n",
    "import os\n",
    "from os import path\n",
    "from pypdf import PdfReader, errors\n",
    "import pickle\n",
    "import openai\n",
    "\n",
    "\n",
    "def is_valid_pdf(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            PdfReader(file)\n",
    "    except (Exception, errors.PdfReadError) as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "docs = Docs()\n",
    "# pdf_path = \"pdfs/hedge_fund_letters\"\n",
    "pdf_path = \"pdfs/distributed_systems_papers\"\n",
    "\n",
    "# TODO: PdfReadError: EOF marker not found then just skip\n",
    "# TODO: after downloading PDFs, check if valid, if not delete them\n",
    "\n",
    "for files in os.listdir(pdf_path):\n",
    "    if is_valid_pdf(f\"{pdf_path}/{files}\"):\n",
    "        print(f\"adding {files}\")\n",
    "        try:\n",
    "            docs.add(f\"{pdf_path}/{files}\")\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    else:\n",
    "        print(f\"skipping {files}\")\n",
    "        # docs.add(f\"{pdf_path}/{files}\")\n",
    "\n",
    "with open(\"distributed_systems_papers.docs.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(docs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from paperqa import Docs\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "docs = Docs()\n",
    "\n",
    "with open(\"hedge_fund_letters.docs.pickle\", \"rb\") as handle:\n",
    "    docs = pickle.load(handle)\n",
    "\n",
    "answer = docs.query(\n",
    "    \"Return a breakdown of invested stocks sorted by the hedge fund's name.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = docs.query(\"what stocks did most of the funds invest in?\")\n",
    "print(answer.formatted_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from paperqa import Docs\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "docs = Docs()\n",
    "\n",
    "with open(\"spx_options_papers.docs.pickle\", \"rb\") as handle:\n",
    "    docs = pickle.load(handle)\n",
    "\n",
    "answer = docs.query(\"how does vix affect SPX options?\")\n",
    "print(answer.formatted_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textualize-paper-qa-FwPdbxm8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67ed05bfd150801523d90acfeb15eb4a8e774f2bbda59b1e2d62b30fb2d9901d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
